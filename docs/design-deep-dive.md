# ğŸ”¬ Design deep dive

The high level design covered two flows the short URL generation and the redirection flow. Here, we discuss those topics
in more detail comparing different approaches and their trade-offs. 

## ğŸ§  Short URL generation
There are multiple ways to generate a short URL from a long URL. The objective being to select an algorithm that:
* ğŸ’ Minimizes collisions.
* ğŸ’ Is scalable and stateless.
* ğŸ’ Produces reasonably short URLs - URLs should be as short as possible. 
* ğŸ’ Support millions of users with low latency.

### ğŸ”¢ Base62 length and capacity analysis
To support 100 million unique short URLs per day without collision, we need to ensure the keyspace is large enough. At 
100 million URLs per day, with Base62 character set (62 characters) the short URL should be at least 7 characters

For e.g: https://tinyurl-like-app.com/abc123y

### ğŸ§  Options for short URL generation

#### ğŸ§® Auto-increment counter
Use a centralized database or distributed counter to generate a sequential numeric ID (e.g., 1, 2, 3...), then encode 
it into Base62 to produce a short URL.

#### ğŸ” Hash of Long URL (SHA-256, MD5, etc.)
Compute a deterministic hash of the long URL using a cryptographic hash function, and use the hash (or a truncated version) 
as the short code (Base62-encoded).

#### ğŸ² Random String Generation
Generate a random Base62 string (e.g., 7â€“9 characters) for each new short URL. Store it alongside the long URL, 
checking for collisions before insertion.

#### â±ï¸ Twitter Snowflake + Base62 âœ…
Generate a globally unique 64-bit ID using a [Snowflake algorithm] (https://en.wikipedia.org/wiki/Snowflake_ID) (based 
on timestamp, machine ID, etc.), then encode it into Base62 to produce a compact short code.

#### ğŸ“Š Comparative Analysis

| Criteria / NFR                           	  | Counter                       	  | Hash (SHA-256/MD5)                 	  | Random String             	  | Snowflake + Base62                   	  |
|---------------------------------------------|----------------------------------|---------------------------------------|------------------------------|-----------------------------------------|
| <br>Unique Mapping                       	  | âœ… Yes                         	  | âœ… Yes                              	  | âœ… Yes <br>(probabilistic) 	  | âœ… Yes <br>(guaranteed)               	  |
| ğŸ“ˆ Analytics Support                      	 | âœ… Yes                         	  | âœ… Yes                              	  | âœ… Yes                     	  | âœ… Yes                                	  |
| ğŸŒ Global Access                          	 | âš ï¸ Central <br>bottleneck      	 | âœ… Stateless                        	  | âœ… Stateless               	  | âœ… Stateless                          	  |
| â˜ï¸ High Availability                      	 | âš ï¸ Requires HA DB              	 | âœ… Stateless                        	  | âœ… Stateless               	  | âœ… Stateless                          	  |
| ğŸ“ Scalability (500M DAU)                 	 | âŒ Scaling DB <br>is hard      	  | âœ… High                             	  | âœ… High                    	  | âœ… Designed for <br>scale             	  |
| âš¡ Low Latency                            	  | âŒ DB latency                  	  | âš ï¸ Hash calc can be <br>slow        	 | âœ… Fast                    	  | âœ… Fast                               	  |
| ğŸ§± Fault Tolerance                        	 | âŒ Single point of <br>failure 	  | âœ… Yes                              	  | âœ… Yes                     	  | âœ… Yes                                	  |
| âœ‚ï¸ Shortness of ID (Base62 length)        	 | âœ… Very short <br>(6â€“7 chars)  	  | âŒ 22â€“43 chars                      	  | âœ… Tunable (6+)            	  | âš ï¸ 11 chars                           	 |
| ğŸ’¥ Collision Risk                         	 | âœ… None                        	  | âš ï¸ Requires truncation <br>handling 	 | âš ï¸ Probabilistic           	 | âœ… None (designed for <br>uniqueness) 	  |
| ğŸ¯ Determinism (same input â†’ same output) 	 | âŒ No                          	  | âœ… Yes                              	  | âŒ No                      	  | âŒ No                                 	  |
| ğŸ“ˆ Sortability (time-ordered)             	 | âŒ No                          	  | âŒ No                               	  | âŒ No                      	  | âœ… Yes                                	  |
| ğŸ§© Infrastructure Simplicity              	 | âŒ DB needed                   	  | âœ… Stateless                        	  | âœ… Stateless               	  | âœ… Stateless                          	  |

### âœ… Conclusion
Considering the above analysis, the best approach for generating short URLs is **Snowflake + Base62** despite the 
slightly longer ID length. The trade-off of 11 characters vs 6-7 is more than compensated:
* ğŸ’ Statelessness
* ğŸ’ No collisions
* ğŸ’ High throughput
* ğŸ’ Ease of sharding
* ğŸ’ Time-ordering (useful for analytics)

## ğŸ§± Database design
The system requires two main data models:
* URL Mappings
* User Information

### ğŸ”— URL Mappings (Short URL â†’ Long URL)
This is a classic key-value use case, where the short URL is the key and the long URL is the value. Given the 
system's scale â€” 100 million new short URLs per day, with each short URL accessed ~1000 times daily â€” **the read-to-write 
ratio is approximately 10:1**.

To support this high-volume, write-heavy workload with predictable low latency:
* **Primary Store**: Use a distributed key-value store such as DynamoDB or Cassandra:
  * Highly available and horizontally scalable
  * Supports TTL (time-to-live) for **auto-expiring short URLs (especially with DynamoDB)**

* **Cache Layer**: Use Redis to cache frequently accessed short URLs and reduce database load.


### ğŸ§®  Redis Memory Estimation

Approximate size per entry:
```
Key + Value + Overhead â‰ˆ 11 + 1000 + 100 = ~1.1 KB
```
Total for 100M URLs/day:
```
100M * 1.1 KB â‰ˆ 110 GB
```
In total, Redis memory required for 1OOM URLs/day is around 110 GB. The cache would be useful for the most frequently
accessed URLs, which can be estimated to be around 10% of the total URLs. This means that we would need around 11 GB of
memory for the cache. 

### ğŸ‘¤ User Information
User data â€” including email, hashed passwords, and preferences â€” requires strong consistency, relational querying, and 
transactional guarantees. A relational database such as PostgreSQL or MySQL is well-suited for this purpose, offering 
full **ACID compliance, **robust indexing, and transaction support**, making it ideal for managing authentication and 
user profiles. To support scalability and high availability:

* **Sharding** can be applied using the user ID or email as the partition key to ensure even data distribution.
* **Read replicas** can be introduced to offload read traffic and enhance fault tolerance.

